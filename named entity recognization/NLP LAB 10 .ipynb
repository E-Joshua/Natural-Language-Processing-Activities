{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joshua E (225229117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=\"Rajkumar said on Monday that WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of abroken trust that Africa-Amercans felt and said responsibility for repairing generations miscommunication and mistrust fell to law enforcement\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rajkumar said on Monday that WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of abroken trust that Africa-Amercans felt and said responsibility for repairing generations miscommunication and mistrust fell to law enforcement'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [WinError\n",
      "[nltk_data]     10060] A connection attempt failed because the\n",
      "[nltk_data]     connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading words: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Rajkumar/NNP)\n",
      "  said/VBD\n",
      "  on/IN\n",
      "  Monday/NNP\n",
      "  that/IN\n",
      "  (ORGANIZATION WASHINGTON/NNP)\n",
      "  --/:\n",
      "  In/IN\n",
      "  the/DT\n",
      "  wake/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  string/NN\n",
      "  of/IN\n",
      "  abuses/NNS\n",
      "  by/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  police/NN\n",
      "  officers/NNS\n",
      "  in/IN\n",
      "  the/DT\n",
      "  1990s/CD\n",
      "  ,/,\n",
      "  (PERSON Loretta/NNP E./NNP Lynch/NNP)\n",
      "  ,/,\n",
      "  the/DT\n",
      "  top/JJ\n",
      "  federal/JJ\n",
      "  prosecutor/NN\n",
      "  in/IN\n",
      "  (GPE Brooklyn/NNP)\n",
      "  ,/,\n",
      "  spoke/VBD\n",
      "  forcefully/RB\n",
      "  about/IN\n",
      "  the/DT\n",
      "  pain/NN\n",
      "  of/IN\n",
      "  abroken/JJ\n",
      "  trust/NN\n",
      "  that/IN\n",
      "  Africa-Amercans/NNP\n",
      "  felt/VBD\n",
      "  and/CC\n",
      "  said/VBD\n",
      "  responsibility/NN\n",
      "  for/IN\n",
      "  repairing/VBG\n",
      "  generations/NNS\n",
      "  miscommunication/NN\n",
      "  and/CC\n",
      "  mistrust/NN\n",
      "  fell/VBD\n",
      "  to/TO\n",
      "  law/NN\n",
      "  enforcement/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "tokens = word_tokenize(sentence1)\n",
    "tags = pos_tag(tokens)\n",
    "ne_trees = ne_chunk(tags)\n",
    "print(ne_trees)\n",
    "\n",
    "ne_trees=ne_chunk(pos_tag(word_tokenize(sentence1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'Rajkumar': 1, 'NNP': 1})]\n",
      "[Counter({'WASHINGTON': 1, 'NNP': 1})]\n",
      "[Counter({'New': 1, 'NNP': 1}), Counter({'York': 1, 'NNP': 1})]\n",
      "[Counter({'Loretta': 1, 'NNP': 1}), Counter({'E.': 1, 'NNP': 1}), Counter({'Lynch': 1, 'NNP': 1})]\n",
      "[Counter({'Brooklyn': 1, 'NNP': 1})]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence1))):\n",
    "    if hasattr(chunk, 'label'):\n",
    "        print([Counter(label) for label in chunk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajkumar', 'WASHINGTON', 'New York', 'police officers', 'Loretta E. Lynch', 'Brooklyn']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence1)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<NN><NNS>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajkumar', 'WASHINGTON', 'the wake', 'a string', 'New York', 'Loretta E. Lynch', 'the top federal prosecutor', 'Brooklyn', 'the pain']\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT><JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rajkumar', 'NNP'), ('said', 'VBD'), ('on', 'IN'), ('Monday', 'NNP'), ('that', 'IN'), ('WASHINGTON', 'NNP'), ('--', ':'), ('In', 'IN'), Tree('NP', [('the', 'DT'), ('wake', 'NN')]), ('of', 'IN'), Tree('NP', [('a', 'DT'), ('string', 'NN')]), ('of', 'IN'), ('abuses', 'NNS'), ('by', 'IN'), ('New', 'NNP'), ('York', 'NNP'), ('police', 'NN'), ('officers', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('1990s', 'CD'), (',', ','), ('Loretta', 'NNP'), ('E.', 'NNP'), ('Lynch', 'NNP'), (',', ','), Tree('NP', [('the', 'DT'), ('top', 'JJ'), ('federal', 'JJ'), ('prosecutor', 'NN')]), ('in', 'IN'), ('Brooklyn', 'NNP'), (',', ','), ('spoke', 'VBD'), ('forcefully', 'RB'), ('about', 'IN'), Tree('NP', [('the', 'DT'), ('pain', 'NN')]), ('of', 'IN'), ('abroken', 'JJ'), ('trust', 'NN'), ('that', 'IN'), ('Africa-Amercans', 'NNP'), ('felt', 'VBD'), ('and', 'CC'), ('said', 'VBD'), ('responsibility', 'NN'), ('for', 'IN'), ('repairing', 'VBG'), ('generations', 'NNS'), ('miscommunication', 'NN'), ('and', 'CC'), ('mistrust', 'NN'), ('fell', 'VBD'), ('to', 'TO'), ('law', 'NN'), ('enforcement', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "parse = cp.parse(tags)\n",
    "print(parse[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rajkumar', 'WASHINGTON', 'the wake', 'a string', 'New York', 'Loretta E. Lynch', 'Brooklyn', 'the pain']\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT><JACJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2=\"European authrities fined google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its parctics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('GPE', [('European', 'JJ')]), ('authrities', 'NNS'), ('fined', 'VBN'), ('google', 'VBP'), ('a', 'DT'), ('record', 'NN'), ('$', '$'), ('5.1', 'CD'), ('billion', 'CD'), ('on', 'IN'), ('Wednesday', 'NNP'), ('for', 'IN'), ('abusing', 'VBG'), ('its', 'PRP$'), ('power', 'NN'), ('in', 'IN'), ('the', 'DT'), ('mobile', 'JJ'), ('phone', 'NN'), ('market', 'NN'), ('and', 'CC'), ('ordered', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('to', 'TO'), ('alter', 'VB'), ('its', 'PRP$'), ('parctics', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "token=word_tokenize(sentence2)\n",
    "tag=nltk.pos_tag(token)\n",
    "ne_tree=ne_chunk(tag)\n",
    "print(ne_tree[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', '5.1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence2)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<CD>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European', 'a record', 'the mobile phone', 'the company']\n"
     ]
    }
   ],
   "source": [
    "word = nltk.word_tokenize(sentence2)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "chunk = nltk.ne_chunk(pos_tag)\n",
    "grammar = \"NP: {<DT><JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(chunk)\n",
    "NE = [ \" \".join(w for w, t in ele) for ele in result if isinstance(ele, nltk.Tree)]\n",
    "print (NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1/2 cups dry red wine\n",
      "3 cloves garlic\n",
      "1 3/4 cups beef broth\n",
      "1 1/4 cups chicken broth\n",
      "1 1/2 tablespoons tomato paste\n",
      "1 bay leaf\n",
      "1 sprig thyme\n",
      "8 ounces bacon cut into 1/4 inch pieces\n",
      "1 tablespoon flour\n",
      "1 tablespoon butter\n",
      "4 1 inch rib-eye steaks\n",
      "1 tablespoon bourbon whiskey\n"
     ]
    }
   ],
   "source": [
    "f=open(\"fr.txt\",'r')\n",
    "file=f.read()\n",
    "file=str(file)\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  1/CD\n",
      "  1/2/CD\n",
      "  cups/NNS\n",
      "  dry/JJ\n",
      "  red/JJ\n",
      "  wine/NN\n",
      "  3/CD\n",
      "  cloves/NNS\n",
      "  garlic/JJ\n",
      "  1/CD\n",
      "  3/4/CD\n",
      "  cups/NNS\n",
      "  beef/VBD\n",
      "  broth/DT\n",
      "  1/CD\n",
      "  1/4/CD\n",
      "  cups/NNS\n",
      "  chicken/VBP\n",
      "  broth/DT\n",
      "  1/CD\n",
      "  1/2/CD\n",
      "  tablespoons/NNS\n",
      "  tomato/VBP\n",
      "  paste/NN\n",
      "  1/CD\n",
      "  bay/NN\n",
      "  leaf/NN\n",
      "  1/CD\n",
      "  sprig/NN\n",
      "  thyme/NN\n",
      "  8/CD\n",
      "  ounces/NNS\n",
      "  bacon/JJ\n",
      "  cut/VBD\n",
      "  into/IN\n",
      "  1/4/CD\n",
      "  inch/NN\n",
      "  pieces/NNS\n",
      "  1/CD\n",
      "  tablespoon/RB\n",
      "  flour/JJ\n",
      "  1/CD\n",
      "  tablespoon/NN\n",
      "  butter/NN\n",
      "  4/CD\n",
      "  1/CD\n",
      "  inch/JJ\n",
      "  rib-eye/JJ\n",
      "  steaks/NNS\n",
      "  1/CD\n",
      "  tablespoon/NN\n",
      "  bourbon/NN\n",
      "  whiskey/NN)\n"
     ]
    }
   ],
   "source": [
    "tokens =nltk.word_tokenize(file)\n",
    "tags =nltk.pos_tag(tokens)\n",
    "ne_trees = ne_chunk(tags)\n",
    "print(ne_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cups', 'dry red wine', 'cloves', 'cups', 'cups', 'tablespoons', 'paste', 'bay leaf', 'sprig thyme', 'ounces', 'inch pieces', 'tablespoon butter', 'inch rib-eye steaks', 'tablespoon bourbon whiskey']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Define the regular expression pattern\n",
    "pattern = 'FOOD: {(<JJ>* <NN.*>+ <IN>)? (<JJ>* <NN.*>+)}'\n",
    "\n",
    "# Tokenize and part-of-speech tag the text\n",
    "text = \"\"\"1 1/2 cups dry red wine\n",
    "3 cloves garlic\n",
    "1 3/4 cups beef broth\n",
    "1 1/4 cups chicken broth\n",
    "1 1/2 tablespoons tomato paste\n",
    "1 bay leaf\n",
    "1 sprig thyme\n",
    "8 ounces bacon cut into 1/4 inch pieces\n",
    "1 tablespoon flour\n",
    "1 tablespoon butter\n",
    "4 1 inch rib-eye steaks\n",
    "1 tablespoon bourbon whiskey\"\"\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Apply the regular expression pattern to the POS tagged tokens\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "tree = cp.parse(pos_tags)\n",
    "\n",
    "# Extract the food recipes from the tree\n",
    "food_recipes = []\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == 'FOOD':\n",
    "        recipe = ' '.join(word for word, tag in subtree.leaves())\n",
    "        food_recipes.append(recipe)\n",
    "\n",
    "# Print the extracted food recipes\n",
    "print(food_recipes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
